{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-01T12:39:40.730607Z","iopub.execute_input":"2021-12-01T12:39:40.731364Z","iopub.status.idle":"2021-12-01T12:39:40.742113Z","shell.execute_reply.started":"2021-12-01T12:39:40.731314Z","shell.execute_reply":"2021-12-01T12:39:40.741089Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Redes Neuronales 2021\n\nIntegrantes de grupo:\n\nMüller, Malena\n\nScala, Tobías \n# TP3: Convolutional Neural Network (CNN) & Transfer Learning\n\nEl trabajo práctico consiste en la predicción de imagenes. El dataset utilizado es el CIFAR-100, el cual consiste de 60 mil imagenes (10 mil para test y 50 mil para train) con una variedad de 100 clases.Las imagenes tienen una resolución de 32x32 píxeles. Los 2 objetivos principales del presente TP son: el diseño de un modelo CNN y transfer learning con Imagenet. \n","metadata":{}},{"cell_type":"markdown","source":"## Se obtiene el data set.\nSe obtiene el dataset presente en Kaggle, el cual es el CIFAR-100. Se normalizan los píxeles dividiendo por 255.","metadata":{}},{"cell_type":"code","source":"trainX = np.load(\"../input/cnn-itba-2021-q2/X_train.npy\")/255\ntrainY = np.load(\"../input/cnn-itba-2021-q2/y_train.npy\")\ntestX = np.load(\"../input/cnn-itba-2021-q2/X_test.npy\")/255\n\nprint(trainX.shape) #para ver cuantas imagenes hay (500 imágenes por cada clase. Hay 100 clases)\nprint(trainY.shape)\nprint(testX.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:39:40.744004Z","iopub.execute_input":"2021-12-01T12:39:40.746151Z","iopub.status.idle":"2021-12-01T12:39:41.266399Z","shell.execute_reply.started":"2021-12-01T12:39:40.746118Z","shell.execute_reply":"2021-12-01T12:39:41.265610Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## Dividimos train entre train y validation.\nEl dataset de train se lo divide en 80% de train y 20% de validation. Esto es para poder evaluar el performance de nuestros modelos con validation antes de predecir el test y luego hacer submit.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrainX, validX, trainY, validY = train_test_split(trainX, trainY, test_size=0.2, random_state=0)\n\n#validX = trainX[40000:,:,:,:]\n#validY = trainY[40000:,:]\n#trainX = trainX[:40000,:,:,:]\n#trainY = trainY[:40000,:]\n\nprint(trainX.shape)\nprint(validX.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:39:41.267637Z","iopub.execute_input":"2021-12-01T12:39:41.267903Z","iopub.status.idle":"2021-12-01T12:39:41.680578Z","shell.execute_reply.started":"2021-12-01T12:39:41.267864Z","shell.execute_reply":"2021-12-01T12:39:41.678627Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"Ploteamos una imagen para observar el contenido de train.","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nplt.figure(figsize=(2,2)) #para cambiar tamaño de imagen\nplt.imshow(trainX[0]) #agarra el elemento 0. Podria poner cualquier numero\nplt.title(str(trainY[0]))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:39:41.687880Z","iopub.execute_input":"2021-12-01T12:39:41.691765Z","iopub.status.idle":"2021-12-01T12:39:41.920521Z","shell.execute_reply.started":"2021-12-01T12:39:41.691687Z","shell.execute_reply":"2021-12-01T12:39:41.919803Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Hacemos data augmentation para \"aumentar\" nuestro dataset.\nConfiguramos las capas que se encargarán de modificar, de forma aleatoria, las características de la imagen (horizontal flip, rotación, zoom y contraste) por cada iteración (epoch). Dado que estas capas se encargan de hacer el data augmentation, nuestro modelo asume que el dataset de train es mucho mayor a lo que es en realidad (40 mil imágenes).","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, RandomZoom, RandomContrast\n\ndataAugmentation = Sequential([RandomFlip(\"horizontal\",input_shape=trainX.shape[1:]),\n                                RandomRotation(0.1),\n                                RandomZoom(0.1),\n                                RandomContrast(0.1)])","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:39:41.925503Z","iopub.execute_input":"2021-12-01T12:39:41.926224Z","iopub.status.idle":"2021-12-01T12:39:42.138269Z","shell.execute_reply.started":"2021-12-01T12:39:41.926156Z","shell.execute_reply":"2021-12-01T12:39:42.137118Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Se diseña un modelo CNN.\nEl modelo CNN implementado respeta las estructuras mencionadas en clase (por ejemplo, VGG). Se generan capas convolucionales seguidos de capas maxpooling. Se agrega una capa dropout a modo de generalizar el dataset de train. La capa flatten es utilizada para generar un 1D vector. Finalmente tenemos una capa densa (fully connected) seguido de la capa de salida con función de activación softmax para la predicción de multiclases.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\n\n#Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1. It is\n#a layer that normalizes its inputs.\n\nNclasses = 100\nmodel = Sequential([dataAugmentation,\n                    Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu'),\n                    MaxPooling2D(pool_size=(2, 2)),\n                    Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'),\n                    MaxPooling2D(pool_size=(2, 2)),\n                    Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'),\n                    MaxPooling2D(pool_size=(2, 2)),\n                    Dropout(0.4),\n                    Flatten(),\n                    Dense(1024, activation='relu'),\n                    #BatchNormalization(),\n                    Dense(Nclasses, activation='softmax')])\n\n#Use this SparseCategoricalCrossentropy loss function when there are two or more label classes. We expect labels to be provided as integers.\n#If you want to provide labels using one-hot representation, please use CategoricalCrossentropy loss.\n\n#Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n\n#Accuracy metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true. This\n#frequency is ultimately returned as binary accuracy: an idempotent operation that simply divides total by count.\n\nmodel.compile(optimizer='adam', loss=SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:39:42.140012Z","iopub.execute_input":"2021-12-01T12:39:42.140242Z","iopub.status.idle":"2021-12-01T12:39:42.354287Z","shell.execute_reply.started":"2021-12-01T12:39:42.140209Z","shell.execute_reply":"2021-12-01T12:39:42.353582Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Entrenamos nuestro modelo.","metadata":{}},{"cell_type":"code","source":"model.fit(trainX, trainY, epochs=30, batch_size=64, verbose=1, workers=-1)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:39:42.355567Z","iopub.execute_input":"2021-12-01T12:39:42.355812Z","iopub.status.idle":"2021-12-01T12:41:20.511560Z","shell.execute_reply.started":"2021-12-01T12:39:42.355768Z","shell.execute_reply":"2021-12-01T12:41:20.510786Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Evalueamos nuestro modelo con validation.","metadata":{}},{"cell_type":"code","source":"model.evaluate(validX, validY)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:41:20.515459Z","iopub.execute_input":"2021-12-01T12:41:20.517082Z","iopub.status.idle":"2021-12-01T12:41:21.873605Z","shell.execute_reply.started":"2021-12-01T12:41:20.517043Z","shell.execute_reply":"2021-12-01T12:41:21.872842Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Se hace transfer learning\nPara implementar tranfer learning, se utilizan los pesos preentrenados de Imagenet. Se utilizará una estructura resnet de 50 capas que contendrá dichos pesos. Cabe mencionar que el modelo imagenet ha sido entrenado por imágenes de una resolución de 256x256 píxeles.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.resnet50 import ResNet50\n\n#include_top: whether to include the fully-connected layer at the top of the network.\n\n#weights: one of None (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded.\n\n#trainable: Boolean, whether the layer's variables (weights) should be trainable (updated).\n\n#La red arranca preentrenada.\nmodelResnet = ResNet50(include_top=False, weights='imagenet', input_shape=(256, 256, 3), classes=100)\nfor layer in modelResnet.layers:\n    if not isinstance(layer, BatchNormalization):\n        layer.trainable = False\n#for layer in modelResnet.layers:\n#    layer.trainable = False ","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:41:21.876711Z","iopub.execute_input":"2021-12-01T12:41:21.877039Z","iopub.status.idle":"2021-12-01T12:41:23.315372Z","shell.execute_reply.started":"2021-12-01T12:41:21.877009Z","shell.execute_reply":"2021-12-01T12:41:23.314501Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.resnet import preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator\n\n#Preprocesses a tensor or Numpy array encoding a batch of images. The images are converted from RGB to BGR, \n#then each color channel is zero-centered with respect to the ImageNet dataset, without scaling.\ntrainX = preprocess_input(trainX)\nvalidX = preprocess_input(validX)\ntestX = preprocess_input(testX)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:41:23.318193Z","iopub.execute_input":"2021-12-01T12:41:23.318865Z","iopub.status.idle":"2021-12-01T12:41:23.744493Z","shell.execute_reply.started":"2021-12-01T12:41:23.318825Z","shell.execute_reply":"2021-12-01T12:41:23.743669Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"Se agregan capas al final para adaptar el modelo preentrenado a nuestro problema (ya que hay pesos que no son entrenables). Como primera capa tenemos el data augmentation explicado anteriormente. Luego se usan capas upsampling para que la resolución para la cual Imagenet fue entrenado (256x256) coincida con la resolución utilizada en CIFAR-100 (32x32). Luego se agrega el modelo preentrenado (resnet50). Se agrega una capa dropout por lo mencionado anteriormente.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Activation, GlobalAveragePooling2D, UpSampling2D\n\n#GlobalAveragePooling2D: Downsamples the input along its spatial dimensions (height and width) by taking the average value over an input\n#window (of size defined by pool_size) for each channel of the input. The window is shifted by strides along each dimension.\n\nmodel_ = Sequential([dataAugmentation,\n                    UpSampling2D(),\n                    UpSampling2D(),\n                    UpSampling2D(),\n                    modelResnet, #Acá está el modelo preentrenado.\n                    GlobalAveragePooling2D(),\n                    Dense(256, activation='relu'),\n                    Dropout(0.4),\n                    BatchNormalization(),\n                    Dense(Nclasses, activation='softmax')])\n\nmodel_.compile(optimizer='adam', loss=SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\nmodel_.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:41:23.745573Z","iopub.execute_input":"2021-12-01T12:41:23.747108Z","iopub.status.idle":"2021-12-01T12:41:24.318768Z","shell.execute_reply.started":"2021-12-01T12:41:23.747064Z","shell.execute_reply":"2021-12-01T12:41:24.317947Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"Entrenamos el modelo preentrenado con el dataset de nuestro problema. Cabe mencionar que con menos epochs, se ha logrado un accuracy mucho mayor que con el modelo no preentrenado.","metadata":{}},{"cell_type":"code","source":"model_.fit(trainX, trainY, epochs=15, batch_size=64, verbose=1, workers=-1)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:41:24.320301Z","iopub.execute_input":"2021-12-01T12:41:24.320591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluemos nuestro modelo preentrenado.","metadata":{}},{"cell_type":"code","source":"model_.evaluate(validX, validY)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predecimos el test.","metadata":{}},{"cell_type":"code","source":"testPred = model_.predict(testX)\ntestPred = testPred.argmax(axis=1) #argmax: Returns the indices of the maximum values along an axis.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preparamos submission.","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(data=testPred, columns=[\"label\"])\ndf.index.name=\"Id\"\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv(\"submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}